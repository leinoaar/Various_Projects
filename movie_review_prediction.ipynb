{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TurkuNLP/intro-to-nlp/blob/master/course_project_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucyWlC5gbOyR"
      },
      "source": [
        "# Introduction to HLT Project\n",
        "\n",
        "- Student(s) Name(s):Aaron Leino\n",
        "- Date: 6.5.2025\n",
        "- Chosen Corpus: Stanford Sentiment Treebank (SST-2)\n",
        "\n",
        "\n",
        "### Corpus information\n",
        "\n",
        "- Description of the chosen corpus: The Stanford Sentiment Treebank contains movie review sentences from Rotten Tomatoes. The reviews are annotated with binary sentiment labels: positive and negative.\n",
        "- Paper(s) and other published materials related to the corpus: Socher et al., 2013: Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank\n",
        "- State-of-the-art performance (best published results) on this corpus: T5-11B and MT-DNN-SMART, both with 97.5% accuracy. (Papers With Code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5d-9uxrcDY-"
      },
      "source": [
        "---\n",
        "\n",
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "caHHQoqEcG1J"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\leino\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import load_dataset\n",
        "import optuna\n",
        "import random, numpy as np\n",
        "import pandas as pd\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovUapilSb8iT"
      },
      "source": [
        "---\n",
        "\n",
        "## 2. Data download and preprocessing\n",
        "\n",
        "### 2.1. Download the corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "PDx40YyzbGPc"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"glue\", \"sst2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXb7CQNCbZOI"
      },
      "source": [
        "### 2.2. Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "RO5BXCuRbYKr"
      },
      "outputs": [],
      "source": [
        "\n",
        "SEED = 2025\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "g = torch.Generator()\n",
        "g.manual_seed(SEED)\n",
        "texts = dataset[\"train\"][\"sentence\"][:8000]\n",
        "labels = dataset[\"train\"][\"label\"][:8000]\n",
        "\n",
        "X_train_texts, X_test_texts, y_train, y_test = train_test_split(\n",
        "    texts, labels, test_size=0.1, random_state=2025\n",
        ")\n",
        "\n",
        "X_val_texts = dataset[\"validation\"][\"sentence\"][:800]\n",
        "y_val = dataset[\"validation\"][\"label\"][:800]\n",
        "\n",
        "vectorizer = CountVectorizer(binary=True, max_features=10000)\n",
        "X_train_vec = vectorizer.fit_transform(X_train_texts)\n",
        "X_test_vec = vectorizer.transform(X_test_texts)\n",
        "X_val_vec = vectorizer.transform(X_val_texts)\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train_vec.toarray(), dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test_vec.toarray(), dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "X_val_tensor = torch.tensor(X_val_vec.toarray(), dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
        "\n",
        "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=32, shuffle=True, generator=g)\n",
        "val_loader = DataLoader(TensorDataset(X_val_tensor, y_val_tensor), batch_size=32, generator=g)\n",
        "test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=32, generator=g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1ntHh_JbrAg"
      },
      "source": [
        "---\n",
        "\n",
        "## 3. Machine learning model\n",
        "\n",
        "### 3.1. Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "Hs2Bf49zbn5C"
      },
      "outputs": [],
      "source": [
        "# I mixed some layers to get non-linearity to the model. Also dropout layers are added to reduce overfitting.\n",
        "\n",
        "class DeepBoWClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=256, hidden_dim2=128, output_dim=2, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.tanh1 = nn.Tanh()\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim2)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.output = nn.Linear(hidden_dim2, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.tanh1(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = self.relu2(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        return self.output(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlO8RVuHcmAh"
      },
      "source": [
        "### 3.2 Hyperparameter optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "IzDrTDd0cWOG"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-07 15:21:53,147] A new study created in memory with name: no-name-201fa385-20ee-49f5-9e75-ad1a9d1ca2a1\n",
            "[I 2025-05-07 15:21:56,130] Trial 0 finished with value: 0.7725 and parameters: {'hidden_dim': 124, 'hidden_dim2': 231, 'dropout': 0.47304225595460103, 'lr': 0.0007782808205273017}. Best is trial 0 with value: 0.7725.\n",
            "[I 2025-05-07 15:22:01,197] Trial 1 finished with value: 0.75125 and parameters: {'hidden_dim': 238, 'hidden_dim2': 89, 'dropout': 0.3629470341884151, 'lr': 0.0009665712540273478}. Best is trial 0 with value: 0.7725.\n",
            "[I 2025-05-07 15:22:11,392] Trial 2 finished with value: 0.7625 and parameters: {'hidden_dim': 496, 'hidden_dim2': 212, 'dropout': 0.28208211408382994, 'lr': 0.004000517487488018}. Best is trial 0 with value: 0.7725.\n",
            "[I 2025-05-07 15:22:13,402] Trial 3 finished with value: 0.76875 and parameters: {'hidden_dim': 82, 'hidden_dim2': 205, 'dropout': 0.10126844670211771, 'lr': 0.00038514014014473015}. Best is trial 0 with value: 0.7725.\n",
            "[I 2025-05-07 15:22:20,367] Trial 4 finished with value: 0.78 and parameters: {'hidden_dim': 338, 'hidden_dim2': 237, 'dropout': 0.22004600189785464, 'lr': 0.0003141935558365935}. Best is trial 4 with value: 0.78.\n",
            "[I 2025-05-07 15:22:27,408] Trial 5 finished with value: 0.78125 and parameters: {'hidden_dim': 363, 'hidden_dim2': 254, 'dropout': 0.2873081624297713, 'lr': 0.00017643094449433023}. Best is trial 5 with value: 0.78125.\n",
            "[I 2025-05-07 15:22:36,949] Trial 6 finished with value: 0.75875 and parameters: {'hidden_dim': 475, 'hidden_dim2': 244, 'dropout': 0.21107894932168306, 'lr': 0.0010947309020486401}. Best is trial 5 with value: 0.78125.\n",
            "[I 2025-05-07 15:22:39,856] Trial 7 finished with value: 0.77 and parameters: {'hidden_dim': 133, 'hidden_dim2': 35, 'dropout': 0.22969728256379646, 'lr': 0.009589520181770766}. Best is trial 5 with value: 0.78125.\n",
            "[I 2025-05-07 15:22:45,841] Trial 8 finished with value: 0.765 and parameters: {'hidden_dim': 294, 'hidden_dim2': 229, 'dropout': 0.12695829866368788, 'lr': 0.00037009010557732775}. Best is trial 5 with value: 0.78125.\n",
            "[I 2025-05-07 15:22:51,622] Trial 9 finished with value: 0.7775 and parameters: {'hidden_dim': 274, 'hidden_dim2': 203, 'dropout': 0.46904471069152753, 'lr': 0.0006110088688662721}. Best is trial 5 with value: 0.78125.\n",
            "[I 2025-05-07 15:23:00,487] Trial 10 finished with value: 0.7825 and parameters: {'hidden_dim': 402, 'hidden_dim2': 133, 'dropout': 0.3635299304716154, 'lr': 0.00011357129821916265}. Best is trial 10 with value: 0.7825.\n",
            "[I 2025-05-07 15:23:09,029] Trial 11 finished with value: 0.785 and parameters: {'hidden_dim': 404, 'hidden_dim2': 139, 'dropout': 0.38300921343114286, 'lr': 0.00011931140431446412}. Best is trial 11 with value: 0.785.\n",
            "[I 2025-05-07 15:23:17,511] Trial 12 finished with value: 0.7875 and parameters: {'hidden_dim': 409, 'hidden_dim2': 146, 'dropout': 0.37794498762948087, 'lr': 0.00010236603042832742}. Best is trial 12 with value: 0.7875.\n",
            "[I 2025-05-07 15:23:26,879] Trial 13 finished with value: 0.7775 and parameters: {'hidden_dim': 434, 'hidden_dim2': 157, 'dropout': 0.3826046436258749, 'lr': 0.00011010239270408982}. Best is trial 12 with value: 0.7875.\n",
            "[I 2025-05-07 15:23:35,297] Trial 14 finished with value: 0.77875 and parameters: {'hidden_dim': 406, 'hidden_dim2': 139, 'dropout': 0.4199006914039487, 'lr': 0.00021305464464959945}. Best is trial 12 with value: 0.7875.\n",
            "[I 2025-05-07 15:23:42,080] Trial 15 finished with value: 0.76375 and parameters: {'hidden_dim': 340, 'hidden_dim2': 170, 'dropout': 0.4126511053323361, 'lr': 0.0027231118537404957}. Best is trial 12 with value: 0.7875.\n",
            "[I 2025-05-07 15:23:46,377] Trial 16 finished with value: 0.78625 and parameters: {'hidden_dim': 211, 'hidden_dim2': 110, 'dropout': 0.33633788426387456, 'lr': 0.00018477369735703329}. Best is trial 12 with value: 0.7875.\n",
            "[I 2025-05-07 15:23:50,504] Trial 17 finished with value: 0.7875 and parameters: {'hidden_dim': 199, 'hidden_dim2': 103, 'dropout': 0.3230929279697042, 'lr': 0.00020833417093062327}. Best is trial 12 with value: 0.7875.\n",
            "[I 2025-05-07 15:23:54,158] Trial 18 finished with value: 0.77875 and parameters: {'hidden_dim': 172, 'hidden_dim2': 74, 'dropout': 0.31564078916598826, 'lr': 0.0002565350860322656}. Best is trial 12 with value: 0.7875.\n",
            "[I 2025-05-07 15:23:59,612] Trial 19 finished with value: 0.7725 and parameters: {'hidden_dim': 286, 'hidden_dim2': 61, 'dropout': 0.2594975301832683, 'lr': 0.0005408835917322344}. Best is trial 12 with value: 0.7875.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best trial:\n",
            "  Accuracy: 0.7875\n",
            "  Params: {'hidden_dim': 409, 'hidden_dim2': 146, 'dropout': 0.37794498762948087, 'lr': 0.00010236603042832742}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "sampler = optuna.samplers.TPESampler(seed=SEED)\n",
        "study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
        "def objective(trial):\n",
        "    # hyperparameters\n",
        "    hidden_dim = trial.suggest_int(\"hidden_dim\", 64, 512)\n",
        "    hidden_dim2 = trial.suggest_int(\"hidden_dim2\", 32, 256)\n",
        "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
        "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
        "\n",
        "    model = DeepBoWClassifier(\n",
        "        input_dim=X_train_tensor.shape[1],\n",
        "        hidden_dim=hidden_dim,\n",
        "        hidden_dim2=hidden_dim2,\n",
        "        dropout=dropout\n",
        "    ).to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Training\n",
        "    epochs = 3\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X_val_batch, y_val_batch in val_loader:\n",
        "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
        "            preds = torch.argmax(model(X_val_batch), dim=1)\n",
        "            correct += (preds == y_val_batch).sum().item()\n",
        "            total += y_val_batch.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "#run the search\n",
        "\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "print(\"Best trial:\")\n",
        "print(f\"  Accuracy: {study.best_value:.4f}\")\n",
        "print(f\"  Params: {study.best_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EzCYTnfcrvN"
      },
      "source": [
        "### 3.3. Evaluation on test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "BG7s-yr6crGF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5 - Loss: 152.8010\n",
            "Epoch 2/5 - Loss: 112.8779\n",
            "Epoch 3/5 - Loss: 64.9765\n",
            "Epoch 4/5 - Loss: 42.4331\n",
            "Epoch 5/5 - Loss: 29.4864\n",
            " The Best model's accuracy: 0.8337\n"
          ]
        }
      ],
      "source": [
        "# Extract the best hyperparameters\n",
        "best_params = study.best_params\n",
        "\n",
        "# Define the best model\n",
        "best_model = DeepBoWClassifier(\n",
        "    input_dim=X_train_tensor.shape[1],\n",
        "    hidden_dim=best_params[\"hidden_dim\"],\n",
        "    hidden_dim2=best_params[\"hidden_dim2\"],\n",
        "    dropout=best_params[\"dropout\"]\n",
        ").to(device)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(best_model.parameters(), lr=best_params[\"lr\"])\n",
        "\n",
        "# training \n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    best_model.train()\n",
        "    total_loss = 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = best_model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss:.4f}\")\n",
        "\n",
        "# Evaluation with the test data\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():   # no gradients since this is just evaluation. Thus it is computationally lighter\n",
        "        for X, y in loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            preds = torch.argmax(model(X), dim=1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "    return correct / total\n",
        "\n",
        "test_accuracy = evaluate(best_model, test_loader)\n",
        "print(f\" The Best model's accuracy: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7ylOS8FdYZ5"
      },
      "source": [
        "---\n",
        "\n",
        "## 4. Results and summary\n",
        "\n",
        "### 4.1 Corpus insights\n",
        "\n",
        "The STT-2 dataset is a binary sentiment classification task based on movie reviews. The sentences vary in length, typically between 5 to 40 words, and contain everyday language as well as film-specific terminology. Originally the dataset is really large (67000 training examples) but I subsampled 8000 with 10% test split. The dataset's official test set does not have the real labels so that is why one needed to be splitted. \n",
        "\n",
        "### 4.2 Results\n",
        "\n",
        "I ran 20 Optuna trials to optimize the hyperparameters (hidden_dim, hidden_dim2, dropout and learning rate). The local optimum for these parameters were hidden_dim = 409, hidden_dim = 146, dropout = 0.378 and learning_rate = 1.023×10⁻⁴. This resulted a validation accuracy of 80,75%. After that I evaluated the model on the test set and the model reached 83.37% accuracy.\n",
        "\n",
        "### 4.3 Relation to state of the art\n",
        "\n",
        "The best performing models on STT-2 typically achieve around 95%. So compared to that there is clearly a room for improvement but considering how simple and light the model is, the results were alright. It provides a strong baseline model at least.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Bonus Task\n",
        "\n",
        "### 5.1. Annotating out-of-domain documents\n",
        "\n",
        "I chose 50 news headlines on various topics. The annotation was based if the topic was positive or negative. I tried to chose topics where such labeling were even possible.\n",
        "\n",
        "### 5.2 Conversion into dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "32DU04FndRdM"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'vectorizer' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m texts_bonus = bonus_df[\u001b[33m\"\u001b[39m\u001b[33msentence\u001b[39m\u001b[33m\"\u001b[39m].tolist()\n\u001b[32m      3\u001b[39m labels_bonus= bonus_df[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m].tolist()  \n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m X_bonus_vec    = \u001b[43mvectorizer\u001b[49m.transform(texts_bonus)      \n\u001b[32m      6\u001b[39m X_bonus_tensor = torch.tensor(X_bonus_vec.toarray(), dtype=torch.float32)\n\u001b[32m      7\u001b[39m y_bonus_tensor = torch.tensor(labels_bonus, dtype=torch.long)\n",
            "\u001b[31mNameError\u001b[39m: name 'vectorizer' is not defined"
          ]
        }
      ],
      "source": [
        "bonus_df = pd.read_csv(\"bonus_annotations.csv\")\n",
        "texts_bonus = bonus_df[\"sentence\"].tolist()\n",
        "labels_bonus= bonus_df[\"label\"].tolist()  \n",
        "\n",
        "X_bonus_vec    = vectorizer.transform(texts_bonus)      \n",
        "X_bonus_tensor = torch.tensor(X_bonus_vec.toarray(), dtype=torch.float32)\n",
        "y_bonus_tensor = torch.tensor(labels_bonus, dtype=torch.long)\n",
        "\n",
        "bonus_dataset = TensorDataset(X_bonus_tensor, y_bonus_tensor)\n",
        "bonus_loader  = DataLoader(bonus_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ghO4JemeFKK"
      },
      "source": [
        "### 5.3. Model evaluation on out-of-domain test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "9tzYWQ_zeCYp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Out‐of‐domain accuracy: 0.6200\n"
          ]
        }
      ],
      "source": [
        "bonus_acc = evaluate(best_model, bonus_loader)\n",
        "print(f\"Out‐of‐domain accuracy: {bonus_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XLZlItdePfJ"
      },
      "source": [
        "### 5.4 Bonus task results\n",
        "\n",
        "The model's accuracy on the news headlines were 62% which is compareable to a coin toss. This is understandable since many headlines have contextual meaning rather than word-based meaning. For example \"massive\" usually indicates to positive with movie reviews but a headline can be something like \"massive invasion to ... led to ...\". Also news can be trickier to label since they usually have neutral point of view.\n",
        "\n",
        "### 5.5. Annotated data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = [\n",
        "    (\"Desperate, traumatised people’: Gaza faces wave of looting, theft and violence\", 0),\n",
        "    (\"Pakistan decries ‘act of war’ as it retaliates against India missile attack\", 0),\n",
        "    (\"Weather tracker: Deadly storms in India and huge hailstones in Paris\", 0),\n",
        "    (\"Simone Inzaghi hails Inter for beating ‘best two sides in Europe’ on way to final\", 1),\n",
        "    (\"Huma Bhabha review – ‘Giacometti is a foil to her flamboyance. She is today’s Picasso’\", 1),\n",
        "    (\"‘It’ll be solemn, enshrining his ashes’: statue of Lemmy to be unveiled in his home town of Stoke-on-Trent\", 1),\n",
        "    (\"Russian drone strike caused tens of millions worth of damage to Chornobyl\", 0),\n",
        "    (\"Mirrors, caddies and skinny shelves: 12 space-saving tricks to make small rooms feel bigger\", 1),\n",
        "    (\"US and China to start talks over trade war this week\", 1),\n",
        "    (\"'It's hard to watch' - Solskjaer discusses Man Utd woes\", 0),\n",
        "    (\"Ronaldo's son gets first Portugal Under-15s call-up\", 1),\n",
        "    (\"EU plans to end Russian gas imports by end of 2027\", 1),\n",
        "    (\"Russia accuses Ukraine of drone attack on Moscow days before WW2 parade\", 0),\n",
        "    (\"Home of Ukrainian Eurovision contestant destroyed\", 0),\n",
        "    (\"Smokey Robinson accused of sexual assault by four women\", 0),\n",
        "    (\"Hours before possible ceasefire begins, Russia and Ukraine launch attacks with two killed in Kyiv\", 0),\n",
        "    (\"OpenAI says non-profit will remain in control after backlash\", 1),\n",
        "    (\"The people refusing to use AI\", 0),\n",
        "    (\"Trump criticised after posting AI image of himself as Pope\", 0),\n",
        "    (\"'It doesn't stick to the rules': The reason Sinners has become a true box-office sensation\", 1),\n",
        "    (\"India Strikes Pakistan but Is Said to Have Lost Jets\", 0),\n",
        "    (\"Welcome to Reno, the Mighty Mecca of All-You-Can-Eat Sushi\", 1),\n",
        "    (\"Live Updates: Conclave to Elect New Pope Is Set to Begin\", 1),\n",
        "    (\"Gazans Despair After Israel Announces More Displacement\", 0),\n",
        "    (\"A Half-Ton Spacecraft Lost by the Soviets in 1972 Is Coming Home\", 1),\n",
        "    (\"A Mother and Father Were Deported. What Happened to Their Toddler?\", 0),\n",
        "    (\"The New York Nonprofit Where Generations of Artists Got Their Start\", 1),\n",
        "    (\"National African American Museum Faces Uncertainty Without Its Leader\", 0),\n",
        "    (\"‘Ragtime’ Is Returning to Broadway\", 1),\n",
        "    (\"‘Forbidden Games’: A War Orphan’s Sweet, Ultimately Shattering Story\", 0),\n",
        "    (\"India strikes Pakistan to avenge a terrorist attack\", 0),\n",
        "    (\"America may be just weeks away from a mighty economic shock\", 0),\n",
        "    (\"AI models could help negotiators secure peace deals\", 1),\n",
        "    (\"Trump’s Ukraine ceasefire is slipping away\", 0),\n",
        "    (\"Chinese military exercises foreshadow a blockade of Taiwan\", 0),\n",
        "    (\"American tariffs are starting to hammer Chinese exporters\", 0),\n",
        "    (\"Russian inflation is too high. Does that matter?\", 0),\n",
        "    (\"Three charts show that America’s imports are booming\", 1),\n",
        "    (\"How a dollar crisis would unfold\", 0),\n",
        "    (\"The success of Ivory Coast is Africa’s best-kept secret\", 1),\n",
        "    (\"For media companies, news is becoming a toxic asset\", 0),\n",
        "    (\"How Donald Trump might steal Christmas\", 0),\n",
        "    (\"A new way to recycle plastic is here\", 1),\n",
        "    (\"Trump’s tariffs will pummel Vietnam\", 0),\n",
        "    (\"America is at risk of a Trumpian economic slowdown\", 0),\n",
        "    (\"Narendra Modi is struggling to boost Indian growth\", 0),\n",
        "    (\"Economic bright spots are getting harder to find in Thailand\", 0),\n",
        "    (\"Snooker targets Brisbane 2032 Olympics to capitalise on Zhao world championship win\", 1),\n",
        "    (\"‘It means everything’: how Union Berlin Women completed epic journey to the top\", 1),\n",
        "    (\"Norwegian fan trades five kilos of fish for ticket to BodO/Glimt v Tottenham\", 1),\n",
        "]\n",
        "df = pd.DataFrame(data, columns=[\"sentence\", \"label\"])\n",
        "df.to_csv(\"bonus_annotations.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
